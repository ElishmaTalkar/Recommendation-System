# AI-Powered Recommendation System

Production-quality recommendation system combining **semantic embeddings**, **hybrid filtering**, and **RAG with Claude** for intelligent reranking.

## ðŸš€ Key Features

- **Semantic Search**: 384-dimensional embeddings using `sentence-transformers/all-MiniLM-L6-v2`
- **pgvector + HNSW**: Sub-10ms similarity search with PostgreSQL vector extension
- **Hybrid Recommender**: Tunable blend of content-based and collaborative filtering (alpha parameter)
- **RAG Pipeline**: Embed â†’ Retrieve â†’ Rerank with Claude â†’ Generate explanations
- **FastAPI**: Async endpoints with Pydantic validation
- **Custom Metrics**: Precision@K, nDCG@K, MAP implemented in pure Python
- **Docker**: One-command deployment with docker-compose

## ðŸ“Š Architecture

```
User Query
    â†“
[Text Preprocessor] â†’ Clean, lemmatize, remove stopwords
    â†“
[Embedding Service] â†’ 384-dim semantic vector
    â†“
[pgvector HNSW] â†’ Top-20 candidates (<10ms)
    â†“
[Hybrid Recommender] â†’ alpha * content + (1-alpha) * collaborative
    â†“
[Claude Reranker] â†’ Intelligent reranking + explanations
    â†“
Results with scores and explanations
```

## ðŸ› ï¸ Tech Stack

| Component | Technology |
|-----------|-----------|
| **Embeddings** | sentence-transformers (all-MiniLM-L6-v2) |
| **Vector DB** | PostgreSQL 16 + pgvector |
| **Indexing** | HNSW (m=16, ef_construction=64) |
| **LLM** | Claude 3 Sonnet (Anthropic) |
| **API** | FastAPI + Pydantic |
| **NLP** | NLTK + spaCy |
| **Data** | Pandas + NumPy |
| **Deployment** | Docker + docker-compose |

## ðŸ“¦ Installation

### Prerequisites

- Docker Desktop (for Mac)
- Python 3.11+
- Anthropic API key (optional, for RAG reranking)

### Quick Start

```bash
# Clone the repository
cd ai-recommendation-system

# Copy environment template
cp .env.example .env

# Edit .env and add your Anthropic API key (optional)
# ANTHROPIC_API_KEY=your_key_here

# Start services with Docker Compose
docker-compose up -d

# The API will be available at http://localhost:8000
```

### Local Development

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Mac/Linux

# Install dependencies
pip install -r requirements.txt

# Download NLP models
python -m spacy download en_core_web_sm
python -c "import nltk; nltk.download('stopwords'); nltk.download('wordnet')"

# Initialize database
python scripts/ingest_data.py --init-db --input data/raw/sample_products.csv --create-index

# Run API
uvicorn src.api.routes:app --reload
```

## ðŸ“š Usage

### 1. Generate Sample Data

```bash
python scripts/generate_sample_data.py
```

### 2. Ingest Data

```bash
python scripts/ingest_data.py \
    --input data/raw/sample_products.csv \
    --init-db \
    --create-index \
    --batch-size 32
```

### 3. Start API

```bash
uvicorn src.api.routes:app --host 0.0.0.0 --port 8000
```

### 4. Test Endpoints

**Item-based recommendations:**
```bash
curl -X POST http://localhost:8000/recommend \
  -H "Content-Type: application/json" \
  -d '{
    "item_id": "ITEM_0001",
    "top_k": 10,
    "alpha": 0.7
  }'
```

**Natural language query:**
```bash
curl -X POST http://localhost:8000/query \
  -H "Content-Type: application/json" \
  -d '{
    "query_text": "comfortable running shoes for marathon training",
    "top_k": 5,
    "use_rag": true
  }'
```

**Health check:**
```bash
curl http://localhost:8000/health
```

## ðŸŽ¯ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | GET | API information |
| `/health` | GET | Health check with service status |
| `/metrics` | GET | System metrics (items, index config) |
| `/recommend` | POST | Hybrid recommendations for an item |
| `/query` | POST | Natural language query with RAG |
| `/ingest` | POST | Add new item to system |
| `/docs` | GET | Interactive API documentation |

## ðŸ“ˆ Benchmarking

Run performance benchmarks:

```bash
python scripts/benchmark.py
```

**Expected Results:**
- Embedding generation: ~20-50ms
- HNSW search: **<10ms** âœ“
- End-to-end query: ~30-60ms

## ðŸ§ª Testing

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html

# Run specific test file
pytest tests/test_preprocessing.py -v
```

## ðŸ”§ Configuration

Edit `.env` file:

```env
# Database
DATABASE_URL=postgresql://recommender:recommender_pass@localhost:5432/recommendations

# API Keys
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Recommender
HYBRID_ALPHA=0.7          # 0=collaborative, 1=content-based
TOP_K_CANDIDATES=20       # Candidates for RAG reranking

# HNSW Index
HNSW_M=16                 # Connections per layer
HNSW_EF_CONSTRUCTION=64   # Build-time search depth
```

## ðŸ“Š Evaluation Metrics

Custom implementations in pure Python:

- **Precision@K**: Fraction of top-K that are relevant
- **Recall@K**: Fraction of relevant items in top-K
- **nDCG@K**: Normalized discounted cumulative gain
- **MAP**: Mean average precision
- **MRR**: Mean reciprocal rank
- **Hit Rate@K**: Binary hit in top-K

## ðŸŽ“ Resume Talking Points

> "I built a production-grade AI recommendation system that combines semantic embeddings with collaborative filtering. I replaced traditional TF-IDF with sentence-transformers and implemented HNSW indexing in PostgreSQL with pgvector, achieving sub-10ms search latencyâ€”a 50x speedup over brute-force. On top of that, I added a full RAG pipeline where user queries are embedded, top candidates are retrieved, and Claude reranks them with natural language explanations. The entire system is containerized with Docker, exposes async FastAPI endpoints, and includes custom evaluation metrics I wrote in pure Python."

**Key Metrics:**
- 384-dimensional semantic embeddings
- <10ms search latency with HNSW
- Hybrid recommender with tunable alpha parameter
- Full RAG pipeline with LLM reranking
- 100% Python, production-ready code

## ðŸ“ Project Structure

```
ai-recommendation-system/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/       # Custom text preprocessor
â”‚   â”œâ”€â”€ embeddings/          # Sentence-transformers service
â”‚   â”œâ”€â”€ database/            # pgvector models & store
â”‚   â”œâ”€â”€ recommender/         # Hybrid recommendation engine
â”‚   â”œâ”€â”€ rag/                 # RAG pipeline with Claude
â”‚   â”œâ”€â”€ api/                 # FastAPI routes & schemas
â”‚   â””â”€â”€ evaluation/          # Custom metrics
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_sample_data.py
â”‚   â”œâ”€â”€ ingest_data.py
â”‚   â””â”€â”€ benchmark.py
â”œâ”€â”€ tests/                   # Unit & integration tests
â”œâ”€â”€ data/                    # Raw & processed data
â”œâ”€â”€ docker-compose.yml       # One-command deployment
â”œâ”€â”€ Dockerfile
â””â”€â”€ requirements.txt
```

## ðŸš¢ Deployment

**Docker Compose (Recommended):**
```bash
docker-compose up -d
```

**Manual:**
```bash
# Start PostgreSQL with pgvector
docker run -d \
  -e POSTGRES_PASSWORD=recommender_pass \
  -p 5432:5432 \
  pgvector/pgvector:pg16

# Run API
uvicorn src.api.routes:app --host 0.0.0.0 --port 8000
```

## ðŸ¤ Contributing

This is a portfolio project demonstrating production-quality Python engineering for AI/ML roles.

## ðŸ“„ License

MIT License

## ðŸ™ Acknowledgments

- **sentence-transformers**: Semantic embedding models
- **pgvector**: PostgreSQL vector extension
- **Anthropic**: Claude API for LLM reranking
- **FastAPI**: Modern Python web framework

---

**Built with â¤ï¸ to showcase production Python skills for AI Engineer roles**
